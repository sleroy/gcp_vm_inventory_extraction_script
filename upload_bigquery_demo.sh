#!/bin/bash
# Script to upload a demo dataset to BigQuery
# Author: Generated by Amazon Q
# Date: 2025-06-03

set -e  # Exit on error

# Configuration
PROJECT_ID=""
DATASET_ID="demo_dataset"
TABLE_ID="vm_inventory"
REGION="us-central1"
SAMPLE_DATA_FILE="sample_vm_data.csv"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Function to check if a command exists
command_exists() {
  command -v "$1" >/dev/null 2>&1
}

# Function to display error and exit
error_exit() {
  echo -e "${RED}ERROR: $1${NC}" >&2
  exit 1
}

# Check prerequisites
echo "Checking prerequisites..."

# Check if gcloud is installed
if ! command_exists gcloud; then
  error_exit "Google Cloud SDK (gcloud) is not installed. Please install it from https://cloud.google.com/sdk/docs/install"
fi

# Check if authenticated
if ! gcloud auth list --filter=status:ACTIVE --format="value(account)" | grep -q "@"; then
  error_exit "Not authenticated with gcloud. Please run 'gcloud auth login' first."
fi

# Get project ID if not specified
if [ -z "$PROJECT_ID" ]; then
  PROJECT_ID=$(gcloud config get-value project)
  if [ -z "$PROJECT_ID" ]; then
    error_exit "No project ID specified. Please set PROJECT_ID in the script or configure a default project with 'gcloud config set project PROJECT_ID'"
  fi
  echo -e "${YELLOW}Using project: $PROJECT_ID${NC}"
fi

# Check if BigQuery API is enabled
echo "Checking if BigQuery API is enabled..."
if ! gcloud services list --project "$PROJECT_ID" | grep -q bigquery.googleapis.com; then
  echo -e "${YELLOW}BigQuery API is not enabled. Enabling now...${NC}"
  gcloud services enable bigquery.googleapis.com --project "$PROJECT_ID" || error_exit "Failed to enable BigQuery API"
fi

# Create sample data
echo "Creating sample VM inventory data..."
cat > "$SAMPLE_DATA_FILE" << EOF
project_id,vm_id,name,zone,status,machine_type,cpu_count,memory_mb,os,creation_timestamp,network,internal_ip,external_ip
$PROJECT_ID,1234567890,vm-web-prod-1,us-central1-a,RUNNING,n1-standard-2,2,7680,Ubuntu 20.04,2025-01-15T10:15:30Z,default,10.0.0.2,34.68.105.21
$PROJECT_ID,1234567891,vm-db-prod-1,us-central1-a,RUNNING,n1-standard-4,4,15360,Debian 11,2025-01-15T10:20:45Z,default,10.0.0.3,
$PROJECT_ID,1234567892,vm-cache-prod-1,us-central1-b,RUNNING,n1-standard-1,1,3840,CentOS 8,2025-01-15T10:25:12Z,default,10.0.0.4,
$PROJECT_ID,1234567893,vm-web-staging-1,us-central1-b,STOPPED,n1-standard-1,1,3840,Ubuntu 20.04,2025-02-20T14:30:00Z,staging,10.0.1.2,34.68.105.22
$PROJECT_ID,1234567894,vm-db-staging-1,us-central1-b,RUNNING,n1-standard-2,2,7680,Debian 11,2025-02-20T14:35:22Z,staging,10.0.1.3,
$PROJECT_ID,1234567895,vm-batch-1,us-central1-c,RUNNING,n1-standard-8,8,30720,Ubuntu 22.04,2025-03-10T09:15:00Z,batch,10.0.2.2,
EOF

echo -e "${GREEN}Sample data created in $SAMPLE_DATA_FILE${NC}"

# Create dataset if it doesn't exist
echo "Creating BigQuery dataset if it doesn't exist..."
if ! bq --project_id="$PROJECT_ID" ls -d | grep -q "$DATASET_ID"; then
  bq --project_id="$PROJECT_ID" mk --dataset --description "Demo VM inventory dataset" --location="$REGION" "$DATASET_ID" || error_exit "Failed to create dataset"
  echo -e "${GREEN}Dataset $DATASET_ID created${NC}"
else
  echo -e "${YELLOW}Dataset $DATASET_ID already exists${NC}"
fi

# Create schema for the table
echo "Creating schema file..."
cat > schema.json << EOF
[
  {"name": "project_id", "type": "STRING", "mode": "REQUIRED", "description": "GCP Project ID"},
  {"name": "vm_id", "type": "STRING", "mode": "REQUIRED", "description": "VM Instance ID"},
  {"name": "name", "type": "STRING", "mode": "REQUIRED", "description": "VM Name"},
  {"name": "zone", "type": "STRING", "mode": "REQUIRED", "description": "Zone where VM is located"},
  {"name": "status", "type": "STRING", "mode": "REQUIRED", "description": "VM Status (RUNNING, STOPPED, etc.)"},
  {"name": "machine_type", "type": "STRING", "mode": "REQUIRED", "description": "VM Machine Type"},
  {"name": "cpu_count", "type": "INTEGER", "mode": "REQUIRED", "description": "Number of vCPUs"},
  {"name": "memory_mb", "type": "INTEGER", "mode": "REQUIRED", "description": "Memory in MB"},
  {"name": "os", "type": "STRING", "mode": "NULLABLE", "description": "Operating System"},
  {"name": "creation_timestamp", "type": "TIMESTAMP", "mode": "REQUIRED", "description": "VM Creation Time"},
  {"name": "network", "type": "STRING", "mode": "NULLABLE", "description": "Network Name"},
  {"name": "internal_ip", "type": "STRING", "mode": "NULLABLE", "description": "Internal IP Address"},
  {"name": "external_ip", "type": "STRING", "mode": "NULLABLE", "description": "External IP Address"}
]
EOF

# Load data into BigQuery
echo "Loading data into BigQuery..."
bq --project_id="$PROJECT_ID" load --source_format=CSV --skip_leading_rows=1 --replace "$DATASET_ID.$TABLE_ID" "$SAMPLE_DATA_FILE" schema.json || error_exit "Failed to load data into BigQuery"

# Verify data was loaded
echo "Verifying data was loaded..."
bq --project_id="$PROJECT_ID" query --nouse_legacy_sql "SELECT COUNT(*) as count FROM \`$PROJECT_ID.$DATASET_ID.$TABLE_ID\`"

# Check if the table exists instead of trying to parse the count
if bq --project_id="$PROJECT_ID" show "$DATASET_ID.$TABLE_ID" &>/dev/null; then
  echo -e "${GREEN}Successfully loaded data into $PROJECT_ID.$DATASET_ID.$TABLE_ID${NC}"
else
  error_exit "Data verification failed. Table not found."
fi

# Clean up temporary files
echo "Cleaning up temporary files..."
rm -f "$SAMPLE_DATA_FILE" schema.json

echo -e "${GREEN}Demo dataset successfully uploaded to BigQuery!${NC}"
echo "You can query it with: bq query --nouse_legacy_sql 'SELECT * FROM \`$PROJECT_ID.$DATASET_ID.$TABLE_ID\` LIMIT 10'"

# Optional: Run a sample query
echo -e "${YELLOW}Running a sample query...${NC}"
bq --project_id="$PROJECT_ID" query --nouse_legacy_sql "SELECT name, machine_type, status, os FROM \`$PROJECT_ID.$DATASET_ID.$TABLE_ID\` LIMIT 5"

echo -e "${GREEN}Done!${NC}"
